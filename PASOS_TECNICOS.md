# ðŸ“‹ Pasos TÃ©cnicos - Pipeline de Procesamiento de Datos de Acero

## ðŸŽ¯ Objetivo
Procesar un archivo de datos comerciales de 113.33 GB comprimido (~373 GB descomprimido) con 2,003,599,864 registros, limpiarlo y cargarlo a BigQuery para anÃ¡lisis EDA.

## ðŸ—ï¸ Arquitectura de la SoluciÃ³n

### Componentes Principales:
1. **Dask** - Procesamiento paralelo de datos grandes
2. **Dataform** - Limpieza y transformaciÃ³n de datos
3. **BigQuery** - Almacenamiento y consulta de datos
4. **Streamlit** - AnÃ¡lisis EDA interactivo
5. **Cloud Shell** - EjecuciÃ³n del pipeline

## ðŸ“ Estructura del Proyecto

```
TestManager/
â”œâ”€â”€ config.py                 # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ data_processor.py         # Script principal de procesamiento
â”œâ”€â”€ dataform.yaml            # ConfiguraciÃ³n de Dataform
â”œâ”€â”€ definitions/
â”‚   â””â”€â”€ clean_data.sql       # Transformaciones SQL de Dataform
â”œâ”€â”€ eda_streamlit.py         # AplicaciÃ³n Streamlit para EDA
â”œâ”€â”€ requirements.txt          # Dependencias de Python
â”œâ”€â”€ run_pipeline.sh          # Script de ejecuciÃ³n en Cloud Shell
â””â”€â”€ PASOS_TECNICOS.md        # Este documento
```

## ðŸš€ Pasos TÃ©cnicos Detallados

### Paso 1: PreparaciÃ³n del Entorno
**Archivo:** `run_pipeline.sh` (lÃ­neas 1-50)

```bash
# Configurar proyecto de Google Cloud
gcloud config set project testmanager-470115

# Habilitar APIs necesarias
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable dataflow.googleapis.com

# Crear bucket y dataset si no existen
gsutil mb -p testmanager-470115 -c STANDARD -l US gs://bucket_acero
bq mk --project_id=testmanager-470115 --location=US dataset_acero
```

**Â¿Por quÃ©?** Necesitamos habilitar las APIs de Google Cloud y crear la infraestructura bÃ¡sica antes de procesar datos.

### Paso 2: Descarga y DescompresiÃ³n
**Archivo:** `data_processor.py` (lÃ­neas 60-80)

```python
def download_and_decompress(self):
    # Usar gsutil para descargar directamente
    cmd = f"gsutil cp {GS_PATH} ./cdo_challenge.csv.gz"
    subprocess.run(cmd, shell=True, check=True)
    
    # Descomprimir
    cmd = "gunzip -f ./cdo_challenge.csv.gz"
    subprocess.run(cmd, shell=True, check=True)
```

**Â¿Por quÃ©?** El archivo estÃ¡ en Google Cloud Storage, por lo que usamos `gsutil` para descargarlo eficientemente y `gunzip` para descomprimirlo.

### Paso 3: Procesamiento con Dask
**Archivo:** `data_processor.py` (lÃ­neas 200-250)

```python
def process_data_in_chunks(self):
    # Leer archivo con Dask
    ddf = dd.read_csv('./cdo_challenge.csv', 
                      blocksize=DASK_CHUNKSIZE,  # 100MB por particiÃ³n
                      dtype={
                          'transaction_id': 'object',
                          'date': 'object',
                          # ... otros tipos de datos
                      })
    
    # Procesar cada particiÃ³n por separado
    for i in range(ddf.npartitions):
        partition = ddf.get_partition(i).compute()
        processed_chunk = self.process_chunk(partition)
        processed_chunks.append(processed_chunk)
```

**Â¿Por quÃ© Dask?**
- **Memoria eficiente**: Divide archivos grandes en particiones manejables
- **Procesamiento paralelo**: Aprovecha mÃºltiples nÃºcleos de CPU
- **Escalabilidad**: Puede manejar archivos de cualquier tamaÃ±o
- **Compatibilidad**: API similar a pandas pero para big data

### Paso 4: Limpieza de Datos
**Archivo:** `data_processor.py` (lÃ­neas 100-150)

#### 4.1 Limpieza de Texto
```python
def clean_text_data(self, text_series):
    # Corregir caracteres corruptos comunes
    text_series = text_series.astype(str).str.replace('ÃƒÂ±', 'Ã±')
    text_series = text_series.str.replace('STÃƒÂ±', 'STÃ‘')
    text_series = text_series.str.replace('UÃƒÂ±ER0ÃƒÂ¢ÃƒÂ¢', 'UÃ‘ER0')
    
    # Normalizar espacios
    text_series = text_series.str.strip()
    text_series = text_series.str.replace(r'\s+', ' ', regex=True)
```

**Â¿Por quÃ©?** Los datos contienen caracteres corruptos como `ÃƒÂ±ÃƒÂ±STÃƒÂ±` que necesitan ser corregidos para legibilidad.

#### 4.2 Limpieza de Fechas
```python
def clean_date_column(self, date_series):
    def parse_date(date_str):
        # Patrones de fecha comunes
        patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # YYYY-MM-DD
            r'(\d{1,2})/(\d{1,2})/(\d{4})',  # MM/DD/YYYY
            r'(\d{1,2})-(\d{1,2})-(\d{4})',  # DD-MM-YYYY
            r'(\d{8})',  # YYYYMMDD
        ]
        
        for pattern in patterns:
            match = re.match(pattern, date_str)
            if match:
                # Convertir segÃºn el patrÃ³n detectado
                return pd.to_datetime(...)
```

**Â¿Por quÃ©?** Las fechas vienen en mÃºltiples formatos y necesitan ser estandarizadas para anÃ¡lisis temporal.

#### 4.3 ConversiÃ³n de Tipos
```python
# Convertir tipos de datos
if 'quantity' in chunk.columns:
    chunk['quantity'] = pd.to_numeric(chunk['quantity'], errors='coerce')

if 'unit_price' in chunk.columns:
    chunk['unit_price'] = pd.to_numeric(chunk['unit_price'], errors='coerce')
```

**Â¿Por quÃ©?** Los datos numÃ©ricos pueden venir como strings y necesitan ser convertidos para cÃ¡lculos matemÃ¡ticos.

### Paso 5: CreaciÃ³n de Columnas Derivadas
**Archivo:** `data_processor.py` (lÃ­neas 180-190)

#### 5.1 Columna AnioMes
```python
# Crear columna AnioMes
if 'date' in chunk.columns:
    chunk['AnioMes'] = chunk['date'].dt.to_period('M').astype(str).str.replace('-', '').astype(int)
```

**Â¿Por quÃ©?** Necesitamos agrupar datos por aÃ±o-mes para anÃ¡lisis temporal y la imputaciÃ³n de precios.

#### 5.2 Columna quantity_null
```python
# Crear columna quantity_null
chunk['quantity_null'] = chunk['quantity'].isna().astype(int)
```

**Â¿Por quÃ©?** Necesitamos identificar registros con cantidades nulas para anÃ¡lisis de calidad de datos.

### Paso 6: ImputaciÃ³n de Precios
**Archivo:** `data_processor.py` (lÃ­neas 300-350)

```python
def calculate_unit_price_imput(self, df):
    # Calcular promedios por AnioMes, region y product_id
    avg_by_product = df.groupby(['AnioMes', 'region', 'product_id'])['unit_price'].mean()
    
    # Para casos donde no hay datos por product_id, usar product_category
    avg_by_category = df.groupby(['AnioMes', 'region', 'product_category'])['unit_price'].mean()
    
    # Aplicar imputaciÃ³n jerÃ¡rquica
    for idx in df[null_mask].index:
        # 1. Intentar con product_id
        # 2. Fallback a product_category
        # 3. Fallback a regiÃ³n y aÃ±o-mes
        # 4. Ãšltimo recurso: promedio general
```

**Â¿Por quÃ©?** Los precios nulos necesitan ser imputados para anÃ¡lisis financiero completo, usando la jerarquÃ­a mÃ¡s especÃ­fica disponible.

### Paso 7: Carga a BigQuery
**Archivo:** `data_processor.py` (lÃ­neas 400-500)

```python
def upload_to_bigquery(self, df, table_name):
    # Configurar job
    job_config = bigquery.LoadJobConfig(
        write_disposition=BQ_WRITE_DISPOSITION,
        schema_update_options=[
            bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION
        ]
    )
    
    # Subir datos en lotes
    for i in range(0, total_rows, batch_size):
        batch_df = df.iloc[i:end_idx]
        
        job = self.client.load_table_from_dataframe(
            batch_df, table_id, job_config=job_config
        )
        job.result()  # Esperar a que termine
```

**Â¿Por quÃ©?** BigQuery tiene lÃ­mites de tamaÃ±o de carga, por lo que dividimos los datos en lotes manejables.

### Paso 8: Limpieza Adicional con Dataform
**Archivo:** `definitions/clean_data.sql`

```sql
WITH cleaned_data AS (
  SELECT
    -- Limpiar transaction_id
    COALESCE(TRIM(transaction_id), 'UNKNOWN') as transaction_id,
    
    -- Limpiar y validar fechas con mÃºltiples formatos
    CASE 
      WHEN SAFE.PARSE_DATE('%Y-%m-%d', date) IS NOT NULL THEN SAFE.PARSE_DATE('%Y-%m-%d', date)
      WHEN SAFE.PARSE_DATE('%m/%d/%Y', date) IS NOT NULL THEN SAFE.PARSE_DATE('%m/%d/%Y', date)
      -- ... mÃ¡s formatos
    END as date,
    
    -- Normalizar segmentos de cliente (priorizar espaÃ±ol)
    CASE 
      WHEN LOWER(customer_segment) IN ('empresarial', 'empresa', 'business', 'corporate') THEN 'Empresarial'
      WHEN LOWER(customer_segment) IN ('individual', 'persona', 'personal', 'retail') THEN 'Individual'
      -- ... mÃ¡s mapeos
    END as customer_segment
```

**Â¿Por quÃ© Dataform?**
- **Consistencia**: Asegura que las transformaciones se apliquen uniformemente
- **Versionado**: Control de versiones de las transformaciones
- **Testing**: Permite validar la calidad de los datos
- **DocumentaciÃ³n**: Auto-documenta las transformaciones

### Paso 9: AnÃ¡lisis EDA con Streamlit
**Archivo:** `eda_streamlit.py`

```python
@st.cache_data(ttl=3600)
def load_data_from_bigquery():
    # Query para obtener muestra representativa
    query = f"""
    SELECT *
    FROM `{PROJECT_ID}.{DATASET}.{TABLE}`
    WHERE date >= DATE_SUB(CURRENT_DATE(), INTERVAL 60 DAY)
    ORDER BY RAND()
    LIMIT 100000
    """
    
    df = client.query(query).to_dataframe()
    return df
```

**Â¿Por quÃ© Streamlit?**
- **Interactividad**: Permite explorar datos de manera dinÃ¡mica
- **VisualizaciÃ³n**: GrÃ¡ficos interactivos con Plotly
- **Performance**: Cache de datos para consultas repetidas
- **Facilidad**: API simple para crear dashboards

## âš¡ Optimizaciones de Rendimiento

### 1. Procesamiento en Chunks
- **TamaÃ±o de chunk**: 100MB para balancear memoria y velocidad
- **ParalelizaciÃ³n**: Procesamiento simultÃ¡neo de mÃºltiples chunks
- **Memoria**: Uso controlado de RAM para evitar crashes

### 2. Carga a BigQuery
- **Batch size**: 1M registros por lote
- **ParalelizaciÃ³n**: MÃºltiples jobs simultÃ¡neos
- **Schema**: DefiniciÃ³n explÃ­cita para evitar inferencia automÃ¡tica

### 3. Cache y OptimizaciÃ³n
- **Streamlit cache**: TTL de 1 hora para datos
- **Dask partitions**: OptimizaciÃ³n automÃ¡tica del tamaÃ±o de particiones
- **BigQuery**: Uso de clustering y particionamiento

## ðŸ” Monitoreo y Logging

### Logging Estructurado
```python
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)
```

### MÃ©tricas de Progreso
```python
if (i + 1) % 10 == 0:
    elapsed = time.time() - self.start_time
    logger.info(f"Procesadas {self.processed_rows:,} filas en {elapsed:.2f} segundos")
```

### VerificaciÃ³n de Resultados
```bash
# Verificar estadÃ­sticas finales
bq query --project_id=$PROJECT_ID --use_legacy_sql=false "
SELECT COUNT(*) as total_rows, AVG(unit_price) as avg_price
FROM \`$PROJECT_ID.$DATASET.$DATAMART_TABLE\`
"
```

## ðŸš¨ Manejo de Errores

### 1. Errores de Descarga
```python
try:
    subprocess.run(cmd, shell=True, check=True)
except Exception as e:
    logger.error(f"Error en descarga: {e}")
    return False
```

### 2. Errores de Procesamiento
```python
try:
    processed_chunk = self.process_chunk(partition)
except Exception as e:
    logger.error(f"Error procesando chunk: {e}")
    return chunk  # Retornar chunk original en caso de error
```

### 3. Errores de BigQuery
```python
try:
    job.result()  # Esperar a que termine
except Exception as e:
    logger.error(f"Error en job de BigQuery: {e}")
    # Reintentar o continuar con siguiente lote
```

## ðŸ“Š MÃ©tricas de Rendimiento Esperadas

### Tiempo de Procesamiento
- **Archivo completo**: 45-60 minutos
- **Por chunk**: 2-5 minutos
- **Carga a BigQuery**: 10-15 minutos

### Uso de Recursos
- **Memoria**: 8GB mÃ¡ximo
- **CPU**: 4 workers paralelos
- **Storage**: 400GB temporal (archivo + procesamiento)

### Calidad de Datos
- **Datos vÃ¡lidos**: >95%
- **Fechas corregidas**: >99%
- **Precios imputados**: >98%

## ðŸ”§ Comandos de EjecuciÃ³n

### 1. Ejecutar Pipeline Completo
```bash
chmod +x run_pipeline.sh
./run_pipeline.sh
```

### 2. Solo Procesamiento de Datos
```bash
python data_processor.py
```

### 3. Solo Limpieza con Dataform
```bash
dataform run --project-id=testmanager-470115 --location=US
```

### 4. Ejecutar AnÃ¡lisis EDA
```bash
streamlit run eda_streamlit.py
```

## ðŸ“ˆ Monitoreo en Tiempo Real

### 1. Logs de Progreso
```bash
tail -f /var/log/syslog | grep "data_processor"
```

### 2. Uso de Recursos
```bash
htop  # Monitorear CPU y memoria
df -h  # Monitorear espacio en disco
```

### 3. Estado de BigQuery
```bash
bq ls --project_id=testmanager-470115 dataset_acero
```

## ðŸŽ¯ Resultados Esperados

### 1. Tablas Creadas
- `acero_table`: Datos procesados bÃ¡sicos
- `datamartclean`: Datos limpios y transformados

### 2. Calidad de Datos
- Caracteres corruptos corregidos
- Fechas estandarizadas
- Precios imputados
- CategorÃ­as normalizadas

### 3. AnÃ¡lisis EDA
- Dashboard interactivo completo
- Visualizaciones de tendencias
- MÃ©tricas de negocio clave
- AnÃ¡lisis de rentabilidad

## ðŸ”„ Mantenimiento y Actualizaciones

### 1. ActualizaciÃ³n de Datos
```bash
# Ejecutar pipeline completo nuevamente
./run_pipeline.sh
```

### 2. ModificaciÃ³n de Transformaciones
```sql
-- Editar definitions/clean_data.sql
-- Ejecutar: dataform run --project-id=testmanager-470115 --location=US
```

### 3. ActualizaciÃ³n de Dependencias
```bash
pip install -r requirements.txt --upgrade
```

## ðŸ“š Recursos Adicionales

- [DocumentaciÃ³n de Dask](https://docs.dask.org/)
- [Dataform Documentation](https://docs.dataform.co/)
- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices)
- [Streamlit Documentation](https://docs.streamlit.io/)

---

**Nota**: Este pipeline estÃ¡ diseÃ±ado para ser robusto y eficiente, manejando archivos de datos muy grandes mientras mantiene la calidad y proporciona insights valiosos para la toma de decisiones empresariales.
