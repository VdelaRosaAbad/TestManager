# ğŸ­ TestManager - Pipeline de Procesamiento de Datos de Acero

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa una soluciÃ³n completa para procesar, limpiar y analizar datos comerciales de una empresa de aceros largos. El objetivo es transformar un dataset de 2 mil millones de registros (~373 GB) en insights accionables para la toma de decisiones empresariales.

## ğŸ¯ Objetivos del DesafÃ­o

- **AnÃ¡lisis EDA**: ExploraciÃ³n exhaustiva de datos comerciales de 5 aÃ±os
- **Limpieza de Datos**: CorrecciÃ³n de caracteres corruptos, fechas y normalizaciÃ³n
- **Estrategia de Rentabilidad**: Propuesta basada en anÃ¡lisis de datos para los prÃ³ximos 12 meses
- **Plan Green Field**: ImplementaciÃ³n desde cero sin capacidades de datos preexistentes

## ğŸ—ï¸ Arquitectura de la SoluciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Google Cloud  â”‚    â”‚   Dask +        â”‚    â”‚   BigQuery +    â”‚
â”‚   Storage       â”‚â”€â”€â”€â–¶â”‚   Python        â”‚â”€â”€â”€â–¶â”‚   Dataform      â”‚
â”‚   (113.33 GB)   â”‚    â”‚   (Procesamiento)â”‚   â”‚   (Limpieza)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   BigQuery      â”‚    â”‚   Streamlit     â”‚
                       â”‚   (Almacenamiento)â”‚  â”‚   (AnÃ¡lisis EDA)â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ CaracterÃ­sticas Principales

### âœ¨ Procesamiento Eficiente
- **Dask**: Procesamiento paralelo de archivos grandes
- **Chunking**: DivisiÃ³n en lotes manejables (100MB)
- **Memoria optimizada**: Uso controlado de RAM (8GB mÃ¡ximo)

### ğŸ§¹ Limpieza Avanzada
- **CorrecciÃ³n de caracteres**: `ÃƒÂ±ÃƒÂ±STÃƒÂ±` â†’ `STÃ‘`
- **NormalizaciÃ³n de fechas**: MÃºltiples formatos â†’ EstÃ¡ndar ISO
- **PriorizaciÃ³n del espaÃ±ol**: CategorÃ­as en espaÃ±ol por defecto
- **ImputaciÃ³n inteligente**: Precios basados en contexto temporal y geogrÃ¡fico

### ğŸ“Š AnÃ¡lisis EDA Completo
- **Dashboard interactivo**: 5 tipos de anÃ¡lisis diferentes
- **Visualizaciones**: GrÃ¡ficos interactivos con Plotly
- **MÃ©tricas de negocio**: KPIs clave para toma de decisiones
- **Cache inteligente**: OptimizaciÃ³n de consultas repetidas

## ğŸ“ Estructura del Proyecto

```
TestManager/
â”œâ”€â”€ ğŸ“„ config.py                 # ConfiguraciÃ³n centralizada
â”œâ”€â”€ ğŸ data_processor.py         # Procesador principal con Dask
â”œâ”€â”€ âš™ï¸ dataform.yaml            # ConfiguraciÃ³n de Dataform
â”œâ”€â”€ ğŸ“ definitions/
â”‚   â””â”€â”€ ğŸ§¹ clean_data.sql       # Transformaciones SQL
â”œâ”€â”€ ğŸ“Š eda_streamlit.py         # Dashboard EDA interactivo
â”œâ”€â”€ ğŸ“¦ requirements.txt          # Dependencias de Python
â”œâ”€â”€ ğŸš€ run_pipeline.sh          # Script de ejecuciÃ³n automÃ¡tica
â”œâ”€â”€ ğŸ“‹ PASOS_TECNICOS.md        # DocumentaciÃ³n tÃ©cnica detallada
â””â”€â”€ ğŸ“– README.md                 # Este archivo
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Dask** | 2023.11.0 | Procesamiento paralelo de big data |
| **Python** | 3.8+ | Lenguaje principal |
| **Dataform** | 1.0.0 | Transformaciones y limpieza de datos |
| **BigQuery** | - | Almacenamiento y consulta de datos |
| **Streamlit** | 1.28.2 | Dashboard interactivo |
| **Plotly** | 5.17.0 | Visualizaciones avanzadas |
| **Google Cloud** | - | Infraestructura en la nube |

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

- **Google Cloud Project** con APIs habilitadas
- **Cloud Shell** o entorno con acceso a gcloud
- **Python 3.8+** con pip
- **Node.js** para Dataform CLI

### 1. Clonar el Repositorio

```bash
git clone https://github.com/VdelaRosaAbad/TestManager.git
cd TestManager
```

### 2. Configurar Google Cloud

```bash
# Configurar proyecto
gcloud config set project testmanager-470115

# Habilitar APIs necesarias
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable dataflow.googleapis.com
```

### 3. Instalar Dependencias

```bash
# Python
pip install -r requirements.txt

# Dataform CLI
npm install -g @dataform/cli
```

### 4. Ejecutar Pipeline Completo

```bash
# Dar permisos de ejecuciÃ³n
chmod +x run_pipeline.sh

# Ejecutar pipeline completo
./run_pipeline.sh
```

## ğŸ“Š Uso del Sistema

### ğŸš€ EjecuciÃ³n del Pipeline

```bash
# Pipeline completo (recomendado)
./run_pipeline.sh

# Solo procesamiento de datos
python data_processor.py

# Solo limpieza con Dataform
dataform run --project-id=testmanager-470115 --location=US
```

### ğŸ“ˆ AnÃ¡lisis EDA

```bash
# Ejecutar dashboard interactivo
streamlit run eda_streamlit.py
```

### ğŸ” VerificaciÃ³n de Resultados

```bash
# Verificar tablas en BigQuery
bq ls --project_id=testmanager-470115 dataset_acero

# Consultar estadÃ­sticas
bq query --project_id=testmanager-470115 --use_legacy_sql=false "
SELECT COUNT(*) as total_rows, AVG(unit_price) as avg_price
FROM \`testmanager-470115.dataset_acero.datamartclean\`
"
```

## ğŸ“‹ Columnas del Dataset Final

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `transaction_id` | STRING | Identificador Ãºnico de transacciÃ³n |
| `date` | DATE | Fecha de la transacciÃ³n (limpia) |
| `timestamp` | DATETIME | Marca de tiempo exacta |
| `customer_id` | STRING | ID del cliente (limpio) |
| `customer_segment` | STRING | Segmento del cliente (espaÃ±ol) |
| `product_id` | STRING | ID del producto (limpio) |
| `product_category` | STRING | CategorÃ­a del producto (espaÃ±ol) |
| `product_lifecycle` | STRING | Estado del producto (espaÃ±ol) |
| `quantity` | INTEGER | Cantidad vendida |
| `unit_price` | FLOAT64 | Precio unitario original |
| `total_amount` | FLOAT64 | Monto total de la transacciÃ³n |
| `currency` | STRING | Moneda (MXN por defecto) |
| `region` | STRING | RegiÃ³n de venta (espaÃ±ol) |
| `warehouse` | STRING | AlmacÃ©n de origen |
| `status` | STRING | Estado de la transacciÃ³n (espaÃ±ol) |
| `payment_method` | STRING | MÃ©todo de pago (espaÃ±ol) |
| `discount_pct` | FLOAT64 | Porcentaje de descuento |
| `tax_amount` | FLOAT64 | Monto de impuestos |
| `notes` | STRING | Notas adicionales |
| `created_by` | STRING | Usuario que creÃ³ el registro |
| `modified_date` | DATE | Fecha de Ãºltima modificaciÃ³n |
| `AnioMes` | INTEGER | AÃ±o-mes en formato YYYYMM |
| `quantity_null` | INTEGER | Indicador de cantidad nula (0/1) |
| `unit_price_imput` | FLOAT64 | Precio imputado para valores nulos |

## âš¡ Optimizaciones de Rendimiento

### ğŸš€ Procesamiento con Dask
- **Particionamiento automÃ¡tico**: 100MB por particiÃ³n
- **Procesamiento paralelo**: 4 workers simultÃ¡neos
- **GestiÃ³n de memoria**: Control automÃ¡tico de RAM

### ğŸ“Š BigQuery
- **Carga en lotes**: 1M registros por lote
- **Schema explÃ­cito**: Evita inferencia automÃ¡tica
- **Clustering**: OptimizaciÃ³n de consultas por regiÃ³n y fecha

### ğŸ¯ Streamlit
- **Cache inteligente**: TTL de 1 hora para datos
- **Muestreo representativo**: 100K registros para anÃ¡lisis
- **Lazy loading**: Carga de datos bajo demanda

## ğŸ“ˆ MÃ©tricas de Rendimiento

| MÃ©trica | Valor Esperado |
|---------|----------------|
| **Tiempo total** | 45-60 minutos |
| **Velocidad de procesamiento** | ~40M registros/minuto |
| **Uso de memoria** | <8GB |
| **Calidad de datos** | >95% |
| **Fechas corregidas** | >99% |
| **Precios imputados** | >98% |

## ğŸ” AnÃ¡lisis EDA Disponibles

### ğŸ“Š Resumen General
- MÃ©tricas clave del negocio
- Calidad de datos
- DistribuciÃ³n por regiones

### ğŸ“… AnÃ¡lisis Temporal
- EvoluciÃ³n de ingresos
- Tendencias de transacciones
- Estacionalidad del negocio

### ğŸ“¦ AnÃ¡lisis de Productos
- Rentabilidad por categorÃ­a
- Top productos
- Ciclo de vida de productos

### ğŸ‘¥ AnÃ¡lisis de Clientes
- SegmentaciÃ³n de clientes
- Top clientes por ingresos
- MÃ©todos de pago preferidos

### ğŸ’° AnÃ¡lisis Financiero
- AnÃ¡lisis de precios
- Estrategias de descuento
- Rentabilidad por regiÃ³n

## ğŸš¨ Manejo de Errores

### ğŸ”„ Reintentos AutomÃ¡ticos
- **Descarga**: Reintento en caso de fallo de red
- **Procesamiento**: ContinuaciÃ³n con chunks siguientes
- **BigQuery**: Reintento de lotes fallidos

### ğŸ“ Logging Detallado
- **Nivel INFO**: Progreso del pipeline
- **Nivel ERROR**: Errores y excepciones
- **MÃ©tricas**: Tiempo y registros procesados

### ğŸ›¡ï¸ ValidaciÃ³n de Datos
- **Integridad**: VerificaciÃ³n de esquemas
- **Calidad**: DetecciÃ³n de anomalÃ­as
- **Consistencia**: ValidaciÃ³n de relaciones

## ğŸ”§ ConfiguraciÃ³n Avanzada

### âš™ï¸ Variables de Entorno

```bash
# ConfiguraciÃ³n del proyecto
export PROJECT_ID="testmanager-470115"
export BUCKET="bucket_acero"
export DATASET="dataset_acero"

# ConfiguraciÃ³n de Dask
export DASK_CHUNKSIZE="100MB"
export DASK_MEMORY_LIMIT="8GB"
```

### ğŸ”§ PersonalizaciÃ³n de Chunks

```python
# En config.py
BATCH_SIZE = 1000000  # 1M registros por lote
MAX_WORKERS = 4       # NÃºmero de workers paralelos
```

## ğŸ“š DocumentaciÃ³n Adicional

- **[Pasos TÃ©cnicos Detallados](PASOS_TECNICOS.md)**: ExplicaciÃ³n paso a paso del pipeline
- **[ConfiguraciÃ³n de Dataform](dataform.yaml)**: ParÃ¡metros de transformaciÃ³n
- **[Transformaciones SQL](definitions/clean_data.sql)**: LÃ³gica de limpieza

## ğŸ¤ ContribuciÃ³n

### ğŸ› Reportar Bugs
1. Crear issue en GitHub
2. Incluir logs de error
3. Describir pasos para reproducir

### ğŸ’¡ Sugerencias
1. Crear issue con etiqueta "enhancement"
2. Describir la funcionalidad deseada
3. Proporcionar casos de uso

### ğŸ”§ Desarrollo Local
1. Fork del repositorio
2. Crear rama feature
3. Implementar cambios
4. Crear Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ“ Soporte

- **Issues**: [GitHub Issues](https://github.com/VdelaRosaAbad/TestManager/issues)
- **DocumentaciÃ³n**: [PASOS_TECNICOS.md](PASOS_TECNICOS.md)
- **Contacto**: Crear issue en el repositorio

## ğŸ¯ Roadmap

### ğŸš€ VersiÃ³n 1.1
- [ ] Dashboard de monitoreo en tiempo real
- [ ] Alertas automÃ¡ticas de calidad de datos
- [ ] API REST para consultas programÃ¡ticas

### ğŸš€ VersiÃ³n 1.2
- [ ] Machine Learning para predicciÃ³n de precios
- [ ] AnÃ¡lisis de sentimiento en notas
- [ ] IntegraciÃ³n con sistemas externos

### ğŸš€ VersiÃ³n 2.0
- [ ] Pipeline de streaming en tiempo real
- [ ] AnÃ¡lisis de geolocalizaciÃ³n
- [ ] Dashboard ejecutivo con KPIs

---

## ğŸ‰ Â¡Gracias por usar TestManager!

Este proyecto demuestra cÃ³mo procesar eficientemente datasets masivos y transformarlos en insights accionables para la toma de decisiones empresariales.

**â­ Si te gusta el proyecto, Â¡dale una estrella en GitHub!**
